\chapter{Background and Literature Review}

\section{Definitions and Descriptions}
Throughout this literature review there are several key natural language processing and machine learning concepts discussed. Rather than defining them as we arrive at them, a list of useful definitions is constructed here.

\subsection*{F1 Score}
The F1 score is a metric used to evaluate predictions. A common concept to find when evaluating binary decisions is a \textit{confusion matrix} from which ROC analysis can be completed \cite{Fawcett2006}. This is a system which records \textit{true positives} and \textit{true negatives} where gold standard and predicted data match, and \textit{false positives} and \textit{false negatives} where gold and predicted data do not match. From this, various values can be calculated. 

Accuracy is one, but often doesn't show the full story as if, on a data set where nine out of ten items are 'false' and just one item is 'true', predicting all false will get an accuracy of 90\%, but no \textit{true positive} occurrences will appear which is not very useful. 

Two better metrics can be calculated, which are precision and recall. They look at the rates of correct and incorrect predictions, and can be combined together to produce an F1 score. This is what ScienceIE's scripts calculate, given ScienceIE's gold standard data and a researchers predictions, comparing instances where the researcher has correctly predicted key phrase boundaries, classifications and relations against the gold standard data.

An F1 score of 1 is perfect, and an F1 score of 0 means nothing was classified correctly at all.

\subsection*{Tokenization}
Tokenizatoin is a simple concept where a document is broken down, from one long string into individual words or symbols. 

\subsection*{Bag-Of-Words Representation}
Bag-of-Words is another simple concept, where each token is considered independently of is semantic meaning. 

\subsection*{Stop Words}
Stop words are words that are commonly filtered out due to their lack of specific meaning and generally do not contribute to useful input, although exceptions can be made if there is little other information. For example, common search engines are likely to ignore the word ``the'' in most searches, but this process of removal needs to be conducted carefully as, for example, when searching for ``the who'', ``the'' is an important part of that query. 

The stop words concept is used in this report, and the list of stop words used is taken from the Stanford CoreNLP GitHub repository\footnote{\href{https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt}{https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt}} and shown in appendix \ref{appendix:stopwords}.

\subsection*{Term Frequency - Inverse Document Frequency (TF-IDF)}
TF-IDF is a metric for assessing how important a word is in a piece of text and has many applications in NLP, including (which will be discussed later) document query \cite{Ramos2003}. It shall also see use throughout this report.

To be calculated for a given token, the document that token came from is required and a set of documents for comparison is required. The set of documents used in this project shall be the ScienceIE training set.

The theory is, a word like ``the'' - which will likely appear many times in almost every document - should a very low TF-IDF score (close to 0). Unusual words however, such as ``xylanases'' which are likely very specific to the paper they are contained in will probably have a much higher TF-IDF value than that of ``the''.

\subsection*{Parse Trees and Part-Of-Speech (POS) Tagging}
A \textit{parse} of a sentence is a tree structure where the root node is a \textit{sentence}, each node below that is a \textit{POS tag} and the leaves of the tree are the words or symbols in the sentence. The POS tag tells us about the semantic meaning of that node and its children - or sub tree - where the POS tag could be \textit{verb phrase} or \textit{noun phrase} for example.

\subsection*{Support Vector Machine (SVM)}
A SVM is a supervised machine learning mechanism \cite{Chih-WeiHsuChih-ChungChang2008}. The input to a SVM is a series of vectors generated from some original input data. Each of these vectors is a set of features - which are simply values calculated based on the original input data. For training, these data points are labelled, indicating their class. Once trained, a SVM can be used to \textit{predict} the label for a new data point.

The training involves attempting to find a well fitting hyperplane with a maximal margin, that separates the labelled data, after mapping that data into a higher dimensional space. This involves an algorithm for finding the distance between the mapped data points, for which a \textit{kernel} can be specified. Furthermore, the hyperplane that is fitted can be allowed to make errors. This is where it allows training data points to be within the hyperplane margins. This, as well as any kernel parameters, can be tuned to increase SVM performance.

\subsection*{Clustering}
Clustering is simply the idea of grouping items together that are similar. The result should be a set of sets of items, where within the group there is a high average similarity, while inter-group similarity is much lower. This is often used for classification and there are a variety of clustering algorithms that can be applied, with various algorithms performing better for various applications \cite{Rai2010}.

\section{ScienceIE Proceedings}
Evaluating the outcome of ScienceIE at SemEval indicates potential paths for future systems and documents very recent activity in the information and key phrase extraction area. Three papers were published from the event regarding this task.

Firstly, an overview of how successful the task was shall be conducted. The highest end-to-end F1 score achieved by any team was measured to be 0.43 for all three sub-systems combined, with each of subtasks A, B and C were 0.56, 0.44 and 0.28 respectively (for end-to-end tests) \cite{Augenstein2017}. Key phrase extraction was considered the hardest part of this set of tasks, shown by the best scores for each task evaluated separately, scoring 0.56, 0.67 and 0.64 respectively.

For subtask A, it was evaluated that while many high scores were achieved with recurrent neural networks, the highest scoring system was a SVM using a well-engineered lexical feature set. SVMs and neural networks were also popular choices for subtask B. For subtask C, many methods were attempted and while a convolutional neural network was the most effective, various other methods (including SVM, multinomial naïve Bayes and Conditional Random Fields (CRFs)) all achieved very similar and reasonably accurate scores.

Furthermore, a common preprocessing technique was to use the spaCy NLP pipeline to analyse the given texts to achieve knowledge about their semantics \cite{Honnibal2015}. Then, the key phrases were directly annotated on to the text model with a flavour of tagging scheme (typically Inside-Outside-Begin or Begin-Inside-Last-Outside-Unit) so that algorithms operating on the produced models have the annotation information to learn from. Algorithms would then predict these tags on the test data, which would then be converted into the BRAT annotation format for evaluation.

The best end-to-end ScienceIE team applied sequence tagging models, which in tern used long short-term memory neural networks and CRFs to solve each subtask separately \cite{Ammar2017}. Their sequence tagging models also employed gazetteers built from scientific words extracted Wikipedia and Freebase, which appear to perform well in the context of scientific papers, as their high results (scoring first or second in all of the scenarios evaluated) indicate the vocabulary supported by these two resources covered much of the test data.

Another contribution also included the use of CRF based models \cite{Marsi2017}. This team actually created a range of CRFs, each with a different intention. They built a range of models, each targeting a specific goal (general key word extraction, task key word extraction, task classification and so on), and each with their own set of features chosen through cross validation. These features generally focussed on the format of the word, the position of the word, and (for material key phrases) comparison to known hyponyms, using WordNet\footnote{\href{https://wordnet.princeton.edu/}{https://wordnet.princeton.edu/}} as a data source. A problem encountered here was that they found some classes had few examples given, so as a solution they removed all sentences that didn't contain a given class (to only focus on areas with positive examples to help balance class distributions). Finally, as there were overlap in the CRF classifiers created, they could be run in parallel and a voting system used to choose a final prediction.

Both of the solution sets above also used sensible rules to help improve their score, such as intuitively marking all instances of a key phrase as a key phrase upon finding one instance (so if \textit{carbon} is extracted and labelled as a \textit{material}, then all other instances in the same document are labelled to match). The teams also exploited hyponym relationship’s bidirectional property (so if word 1 is a hyponym of word 2, word 2 is a hypernym of word 1), meaning their classifier could classify the relation in either direction and the final \textit{hyponym-of} relation could be returned.

The results of ScienceIE demonstrate there are several potential systems that could be implemented to answer this problem, with the best system potentially being a combination of algorithms to select and label key phrases, either directly working together or separately and then their results being passed through some voting system to choose a final prediction. The majority of the solutions presented at ScienceIE involve supervised learning, which requires the use of the training data, with little mention of unsupervised approaches. This suggests the current best solutions require learning from examples, which could cause a problem if the training data is of poor quality or (as one team worked around) there a lack of examples for a given class. Furthermore, as training data is being used, over-fitting needs to be considered, where solutions presented may work well for the test set but not in the real world, but unfortunately it is hard to evaluate the application of the produced algorithms on random scientific articles as the researched ended after testing with ScienceIE data and there is little fully annotated data available.

\section{Further Background}
Now a review of other research is reviewed to explore other contributions to information extraction.

A recent review of information extraction techniques was conducted in 2014 and listed various techniques \cite{Hasan2014}. A popular trend noted was that some systems begin by selecting candidate key phrases via some heuristic before applying any machine learning to the data, which may help reduce the problem complexity if less data needs to be processed. Supervised learning techniques included naïve Bayes, decision trees, bagging, boosting, maximum entropy, and SVMs. Unsupervised approaches used fields such as graph-based ranking, topic clustering, and language modelling to determine important words and phrases. 

Furthermore, the study also discussed evaluation of key phrases: It described the test model used at SemEval 2010 for a very similar key phrase extraction task to Science, and describes the matching between predicted information and gold standard information as \textit{overly strict}. It also discusses manual review, but this, naturally, is time consuming and very expensive to complete. By completing analysis of the errors produced by current key phrase extraction systems, it is described how over-generation (where the key phrase is predicted with extra information attached) is the largest problem at the moment. It does suggest a way to reduce this over-generation problem is to compare generated key phrases to a knowledge base to look for semantically equivalent pieces of information, to aid in the generation of more \textit{to the point} key phrases. ScienceIE's data could extend this further, as the use of hyponym and synonym extraction could be used to exploit the background knowledge about one key phrase to help verify another that there isn't a record found for.

A CRF based key phrase extraction model was completed for use on Chinese documents which saw very high results \cite{Zhang2008}. The CRF model designed contained 22 different features, where almost all of the features were based around the position of the token in the text. This paper does targets a language other than English, so their use of semantics may have different affects if applied in the same way to English sentences. For their test data set, the largest F1 score measured was 0.51, which is very similar to the best scoring mechanism used on the ScienceIE data set. This implies CRF models can be effective when applied to multiple languages (as approaches at ScienceIE used position features heavily as well), but also infers that little progress has been made in the decade since this research, given (although different data sets were used) very similar results. 

Naïve Bayes was used at ScienceIE to great effect, and has been used in the past effectively. A study that utilised naïve Bayes achieved good results by default, but were able to leverage information about the domain of the key phrases to increase their scores \cite{Wu2005}. Using information about the occurrence probabilities of key phrases throughout documents, based on the training set provided, this study shows that this information about the frequency of key phrases can be integrated into an established classifier to boost scores. This study focussed on longer papers with the goal of producing a few key words per scientific paper, similar to that of an author. With the ScienceIE task however, the test data is short documents and they each can contain many key phrases (discussed in detail later). It can still be assumed, however, that any future system created should consider at least the frequency of key phrases in training to improve the rate of key phrase production in test, as there can be large differences in the expected number key phrases.

As mentioned earlier, WordNet is also a potential source of information for building a classifier, although other knowledge bases exist. One study compared building a classifier using WordNet and Wikipedia for hyponym-only extraction \cite{Snow2004}. The concept of the system produced was that it could learn the lexical patterns of example hyponym pairs so that could be used to find hyponym relations. Using Wikipedia scored higher (F1 score of 0.36 on their dataset) than when using WordNet (F1 score of 0.27), showing it may be more suited to creating this type of classifier. While an improvement, it is not ultimately a huge increase and there is no evidence either Wikipedia is more useful for the specific area of scientific papers, as the dataset used in this study was not focussed. However, Wikipedia has more information that WordNet, and would likely cover more of the scientific concepts covered by any ScienceIE data. It is also updated on a more regular basis meaning new definitions are likely to be available much earlier than if using WordNet. With the study being completed some time ago, as Wikipedia matures it may increase or decrease the result of the experiment if repeated, depending upon the quality of the information contained in Wikipedia, which if tested would help evaluate its effectiveness.

A method not attempted at ScienceIE was unsupervised learning by clustering key phrases, a method which has potentially very accurate results that also could not only be robust again new unseen data but even different languages. A study used the idea that words can be clustered together through relatedness, with the goal of producing a set of clusters representing topics in a document \cite{Liu2009}. To extract the key phrases, using the simplest approach, the centre of a cluster is the key phrase. Various clustering methods were attempted by this study. They ran tests on relatively short articles and while at maximum they only achieved an F1 of 0.45, there was several improvements suggested which apply to the ScienceIE task concerning scientific papers. Two suggested covered how the clusters were formed, including clustering on noun groups (which most key phrases at ScienceIE are) and creating a heuristic for co-occurrence based and Wikipedia based relatedness. Another suggestion was to improve the removal of unimportant words. The study only used stop words, but introducing a TF-IDF filter could also be possible.

\section{Word2Vec}
Through recommendations, this project also considers the use of Word2Vec. Word2Vec is an application of neural networks, that processes text and creates a vector space containing each word in the text. This vector space can be used to find the similarity and differences between words \cite{Mikolov2013}. This technology is relatively new (initially designed in 2013) and its uses are still being discovered. A project with a similar goal called GloVe was also created in the following year \cite{Pennington2014}.

Given a large set of input texts, Word2Vec aims to learn the meaning of words. It processes all examples of where every word in the document library has been used contextually, and builds a vector representation of each word. These vector representations can be compared to try to extra the similarity, or relative similarity between words. For example, the relative distance from a \textit{knee} to a \textit{leg} may be similar to that of \textit{elbow} to \textit{arm}. It has been proven to be effective, although very difficult to understand \cite{Goldberg2014}. 

In terms of information extraction, there has been a recent study into its use to find extra hyponym information from text \cite{Nayak2015} using GloVe. This study designed a function based on the GloVe vector space to create a system to predict hyponym pairs. Reasonable results were achieved, scoring up to 35\% precision in some cases. However, a problem discussed is that words can have multiple meanings, which can effectively confuse the model, which decreases its quality for determining similarity and therefore reducing the effectiveness of a system built. 

\section{The Supplied Data Set}
The ScienceIE data set consists of 50 development, 350 training and 100 test documents. This break up is sensible, as it allows for fast approximate results while developing with a small development set, and a large training and testing set to evaluate \textit{close to real world} performance (having a separate training and testing set should reduce over fitting where performance is optimised just for the dataset worked with).

Some analysis conducted at ScienceIE \cite{Augenstein2017} showed some characteristics of the sample key phrases included:
\begin{itemize}
	\item Only 22\% of key phrases had 5 or more tokens,
	\item 93\% of key phrases were noun phrases,
	\item Only 31\% of key phrases seen in the training set were also in the test set.
\end{itemize}

This means that key phrase extraction appears quite difficult, as an algorithm needs to search for short phrases, processing phrases that it likely hasn't seen instances of before. Most of the key phrases being noun phrases, however, is valuable information as it helps to identify a simple feature of tokens to be evaluated.

\begin{table}
	\centering
	\caption[ScienceIE Training Set Analysis]{Key phrase (KP), token and relation analysis for the ScienceIE training set.}
	\begin{tabular}{ c | c c c c }
		& \textbf{Minimum} & \textbf{Average} & \textbf{Maximum} & \textbf{Standard Deviation} \\
		\hline
		Nnumber of KPs & 4 & 19 & 46 & 8 \\
		Minimum tokens per KP & 1 & 1 & 3 & 0.4 \\
		Average tokens per KP & 1 & 3 & 8 & 1 \\
		Maximum tokens per KP & 2 & 9 & 25 & 4 \\
		Number of relations & 0 & 2 & 13 & 2 \\
		Total tokens in document & 60 & 159 & 264 & 46
	\end{tabular}
	\label{table:traininganalysis}
\end{table}

Other useful characteristics about the training set, found during this study, can be seen in table \ref{table:traininganalysis}.

Papers in the ScienceIE data set have many key phrases associated with them. With an average of 19 key phrases per paper, an average of 3 tokens per key phrase (meaning on average 57 key tokens per paper) and the average document containing only 159 tokens in total, around a third of all tokens are part of key phrases. This is partly due to the documents supplied by ScienceIE being very short (all are just one paragraph) and are \textit{extracts} of papers rather than full publications. It is not a problem that the documents for processing are short - in fact that may help as the longer the document, the harder it is to choose key phrases \cite{Hasan2014} - however, it may mean any algorithm created here may not scale well to full scientific papers. It seems the ScienceIE task is looking for localised key phrases, choosing several from one paragraph; while the author of a paper may choose to select just five or ten key phrases from the whole paper. While this project will focus on the ScienceIE task with the given test data, a brief look longer or full papers shall be considered.

\section{Summary}
The above background and literature review has defined some important concepts for the rest of this report, and covered some of the strongest algorithms designed at ScienceIE and ideas developed outside of ScienceIE as well. It has highlighted some of the popular techniques for information extraction currently used, including the features that are useful for this task, which can also be context sensitive. The data available for this task has also been discussed, noting how the documents are short and the key phrases are many.

The main reason for completing this task is that, as discussed at ScienceIE, key phrase extraction is still an open question in information retrieval and probably the hardest out of the three subtasks presented. However, the discussed difficulty encountered when evaluating key phrase extraction may rank some systems poorly when they may actually have achieved a high volume of the target information, just with some extra noise in addition.
